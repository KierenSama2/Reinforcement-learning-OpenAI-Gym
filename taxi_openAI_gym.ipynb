{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym # openAi gym\n",
    "from gym import envs\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "169"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('Taxi-v3')   # Here you set the environment\n",
    "env._max_episode_steps = 40000\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(policy, env, discount_factor=1.0, theta=0.00001):\n",
    "    \"\"\"\n",
    "    Implement the policy evluation algorithm here given a policy and a complete model of the environment.\n",
    "    \n",
    "    \n",
    "    Arguments:\n",
    "        policy: [S, A] shaped matrix representing the policy.\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: This is the minimum threshold for the error in two consecutive iteration of the value function.\n",
    "        discount_factor: This is the discount factor - Gamma.\n",
    "    \n",
    "    Returns:\n",
    "        Vector of length env.nS representing the value function.\n",
    "    \"\"\"\n",
    "#     Start with a random (all 0) value function\n",
    "    V = np.zeros(env.nS)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(env.nS):\n",
    "            vNew = 0\n",
    "            for a in range(env.nA):\n",
    "                for prob, nextState, reward, done in env.P[s][a]:\n",
    "                    vNew+=policy[s][a] * prob * (reward + discount_factor*V[nextState])\n",
    "            \n",
    "            delta = max(delta, np.abs(V[s]-vNew))\n",
    "            V[s] = vNew\n",
    "                \n",
    "#         print(delta)\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    print(V)\n",
    "    \n",
    "    return np.array(V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env, policy_eval_fn=policy_evaluation, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Implement the Policy Improvement Algorithm here which iteratively evaluates and improves a policy\n",
    "    until an optimal policy is found.\n",
    "    \n",
    "    Arguments:\n",
    "        env: The OpenAI envrionment.\n",
    "        policy_eval_fn: Policy Evaluation function that takes 3 arguments:\n",
    "            policy, env, discount_factor.\n",
    "        discount_factor: gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V). \n",
    "        policy is the optimal policy, a matrix of shape [S, A] where each state s\n",
    "        contains a valid probability distribution over actions.\n",
    "        V is the value function for the optimal policy.\n",
    "        \n",
    "    \"\"\"\n",
    "    def one_step_lookahead(state, V):\n",
    "        \"\"\"\n",
    "        Implement the function to calculate the value for all actions in a given state.\n",
    "        \n",
    "        Arguments:\n",
    "            state: The state to consider (int)\n",
    "            V: The value to use as an estimator, Vector of length env.nS\n",
    "        \n",
    "        Returns:\n",
    "            A vector of length env.nA containing the expected value of each action.\n",
    "        \"\"\"\n",
    "        A = np.zeros(env.nA)\n",
    "        for a in range(env.nA):\n",
    "            for prob, nextState, reward, done in env.P[state][a]:\n",
    "                A[a] += prob * (reward + discount_factor * V[nextState])\n",
    "\n",
    "        return A\n",
    "\n",
    "\n",
    "    # Start with a random policy\n",
    "    policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "    \n",
    "    numIterations = 0\n",
    "\n",
    "    while True:\n",
    "        numIterations += 1\n",
    "        \n",
    "        V = policy_eval_fn(policy, env, discount_factor)\n",
    "        policyStable = True\n",
    "        \n",
    "        for s in range(env.nS):\n",
    "            oldAction = np.argmax(policy[s])\n",
    "\n",
    "            qValues = one_step_lookahead(s, V)\n",
    "            newAction = np.argmax(qValues)\n",
    "\n",
    "            if oldAction != newAction:\n",
    "                policyStable = False\n",
    "                        \n",
    "            policy[s] = np.zeros([env.nA])\n",
    "            policy[s][newAction] = 1\n",
    "\n",
    "        if policyStable:\n",
    "            print(numIterations)\n",
    "            return policy, V\n",
    "    \n",
    "    return policy, np.zeros(env.env.nS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "policyPI, valuePI = policy_iteration(env, discount_factor=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " ...\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(policyPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[944.72316569 864.01270478 903.55686813 873.75019718 789.53759087\n",
      " 864.01270478 789.53757231 816.76645272 864.01272333 826.02673924\n",
      " 903.55686813 835.38053503 807.59881631 826.02673924 807.59879776\n",
      " 873.75019718 955.27593403 873.75021631 913.69381566 883.58606752\n",
      " 934.27593403 854.37257773 893.52129945 864.01269521 798.52282815\n",
      " 873.75021631 798.52280978 826.02672968 854.3725961  816.76647185\n",
      " 893.52129945 826.02672968 816.76649022 835.38055415 816.76647185\n",
      " 883.58606752 944.72317469 883.58608645 903.5568775  893.52128998\n",
      " 883.58610464 807.59880713 844.82885195 816.76646238 844.82887014\n",
      " 923.9331565  844.82885195 873.75020684 844.82887014 807.59880713\n",
      " 883.58608645 816.76646238 826.0267668  844.82885195 826.02674861\n",
      " 893.52128998 893.52132673 934.27592494 893.52130873 903.55686813\n",
      " 873.7502436  798.52281906 835.38056343 807.59879776 854.37260501\n",
      " 934.27592494 854.37258701 883.58607708 835.38058144 798.52281906\n",
      " 873.75022559 807.59879776 835.38058144 854.37258701 835.38056343\n",
      " 903.55686813 883.58611347 944.72316569 883.58609564 913.69381566\n",
      " 864.01274116 789.53759087 826.0267578  798.52280978 864.01274116\n",
      " 944.72316569 864.01272333 893.52129945 826.02677562 789.53759087\n",
      " 864.01272333 798.52280978 826.02677562 844.82886114 826.0267578\n",
      " 893.52129945 873.75025233 955.27593403 873.75023468 903.5568775\n",
      " 934.27593403 854.37257773 893.52129945 864.01269521 798.52282815\n",
      " 873.75021631 798.52280978 826.02672968 873.75023468 835.38055415\n",
      " 913.69381566 844.82883302 816.76649022 835.38055415 816.76647185\n",
      " 883.58606752 944.72317469 883.58608645 923.9331565  893.52128998\n",
      " 923.93317469 844.82885195 883.58608645 854.37256826 807.59882532\n",
      " 883.58608645 807.59880713 835.38054469 864.01273234 826.02674861\n",
      " 903.5568775  835.38054469 826.0267668  844.82885195 826.02674861\n",
      " 893.52128998 934.27594294 893.52130873 913.69382494 903.55686813\n",
      " 893.52132673 816.76648112 854.37258701 826.02673924 835.38058144\n",
      " 913.69382494 835.38056343 864.01270478 854.37260501 816.76648112\n",
      " 893.52130873 826.02673924 835.38058144 854.37258701 835.38056343\n",
      " 903.55686813 903.55690451 923.93316569 903.55688669 913.69381566\n",
      " 883.58611347 807.59881631 844.82886114 816.76647185 844.82887896\n",
      " 923.93316569 844.82886114 873.75021631 844.82887896 807.59881631\n",
      " 883.58609564 816.76647185 844.82887896 864.01272333 844.82886114\n",
      " 913.69381566 893.52133547 934.27593403 893.52131782 923.9331565\n",
      " 873.75025233 798.52282815 835.38057252 807.59880713 854.37261375\n",
      " 934.27593403 854.3725961  883.58608645 835.38059017 798.52282815\n",
      " 873.75023468 807.59880713 835.38059017 854.3725961  835.38057252\n",
      " 903.5568775  883.58612211 944.72317469 883.58610464 913.69382494\n",
      " 923.93317469 844.82885195 883.58608645 854.37256826 807.59882532\n",
      " 883.58608645 807.59880713 835.38054469 883.58610464 844.82885195\n",
      " 923.9331565  854.37256826 826.0267668  844.82885195 826.02674861\n",
      " 893.52128998 934.27594294 893.52130873 934.27592494 903.55686813\n",
      " 913.69384294 835.38056343 873.75022559 844.82884258 816.76649913\n",
      " 893.52130873 816.76648112 844.82884258 873.7502436  835.38056343\n",
      " 913.69382494 844.82884258 835.38058144 854.37258701 835.38056343\n",
      " 903.55686813 923.93318351 903.55688669 923.93316569 913.69381566\n",
      " 903.55690451 826.0267578  864.01272333 835.38055415 826.02677562\n",
      " 903.55688669 826.0267578  854.37257773 864.01274116 826.0267578\n",
      " 903.55688669 835.38055415 844.82887896 864.01272333 844.82886114\n",
      " 913.69381566 913.69385168 913.69383403 913.69383403 923.9331565\n",
      " 893.52133547 816.76649022 854.3725961  826.02674861 835.38059017\n",
      " 913.69383403 835.38057252 864.01271415 854.37261375 816.76649022\n",
      " 893.52131782 826.02674861 854.37261375 873.75023468 854.3725961\n",
      " 923.9331565  903.55691316 923.93317469 903.55689569 934.27592494\n",
      " 883.58612211 807.59882532 844.82887014 816.76648112 844.82888761\n",
      " 923.93317469 844.82887014 873.75022559 844.82888761 807.59882532\n",
      " 883.58610464 816.76648112 844.82888761 864.01273234 844.82887014\n",
      " 913.69382494 893.52134403 934.27594294 893.52132673 923.93316569\n",
      " 913.69384294 835.38056343 873.75022559 844.82884258 798.52283706\n",
      " 873.75022559 798.52281906 826.02673924 893.52132673 854.37258701\n",
      " 934.27592494 864.01270478 816.76649913 835.38056343 816.76648112\n",
      " 883.58607708 923.93318351 883.58609564 944.72316569 893.52129945\n",
      " 903.55690451 826.0267578  864.01272333 835.38055415 807.59883414\n",
      " 883.58609564 807.59881631 835.38055415 864.01274116 826.0267578\n",
      " 903.55688669 835.38055415 826.02677562 844.82886114 826.0267578\n",
      " 893.52129945 913.69385168 893.52131782 913.69383403 903.5568775\n",
      " 893.52133547 816.76649022 854.3725961  826.02674861 816.76650787\n",
      " 893.52131782 816.76649022 844.82885195 854.37261375 816.76649022\n",
      " 893.52131782 826.02674861 835.38059017 854.3725961  835.38057252\n",
      " 903.5568775  903.55691316 903.55689569 903.55689569 913.69382494\n",
      " 883.58612211 807.59882532 844.82887014 816.76648112 826.02678427\n",
      " 903.55689569 826.0267668  854.37258701 844.82888761 807.59882532\n",
      " 883.58610464 816.76648112 864.01274981 883.58610464 864.01273234\n",
      " 934.27592494 893.52134403 913.69384294 893.52132673 944.72316569\n",
      " 873.75026089 798.52283706 835.38058144 807.59881631 835.38059873\n",
      " 913.69384294 835.38058144 864.01272333 835.38059873 798.52283706\n",
      " 873.7502436  807.59881631 854.37262231 873.7502436  854.37260501\n",
      " 923.93316569 883.58613059 923.93318351 883.58611347 934.27593403\n",
      " 903.55690451 826.0267578  864.01272333 835.38055415 789.53760869\n",
      " 864.01272333 789.53759087 816.76647185 903.55690451 864.01272333\n",
      " 944.72316569 873.75021631 807.59883414 826.0267578  807.59881631\n",
      " 873.75021631 913.69385168 873.75023468 955.27593403 883.58608645\n",
      " 893.52133547 816.76649022 854.3725961  826.02674861 798.5228458\n",
      " 873.75023468 798.52282815 826.02674861 854.37261375 816.76649022\n",
      " 893.52131782 826.02674861 816.76650787 835.38057252 816.76649022\n",
      " 883.58608645 903.55691316 883.58610464 903.55689569 893.52130873\n",
      " 883.58612211 807.59882532 844.82887014 816.76648112 807.59884279\n",
      " 883.58610464 807.59882532 835.38056343 844.82888761 807.59882532\n",
      " 883.58610464 816.76648112 826.02678427 844.82887014 826.0267668\n",
      " 893.52130873 893.52134403 893.52132673 893.52132673 903.55688669\n",
      " 873.75026089 798.52283706 835.38058144 807.59881631 816.76651643\n",
      " 893.52132673 816.76649913 844.82886114 835.38059873 798.52283706\n",
      " 873.7502436  807.59881631 873.75026089 893.52132673 873.7502436\n",
      " 944.72316569 883.58613059 903.55690451 883.58611347 955.27593403\n",
      " 864.01275828 789.53760869 826.02677562 798.52282815 826.02679275\n",
      " 903.55690451 826.02677562 854.3725961  826.02679275 789.53760869\n",
      " 864.01274116 798.52282815 864.01275828 883.58611347 864.01274116\n",
      " 934.27593403 873.75026928 913.69385168 873.75025233 944.72317469]\n"
     ]
    }
   ],
   "source": [
    "print(valuePI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, theta=0.0001, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    This section is for Value Iteration Algorithm.\n",
    "    \n",
    "    Arguments:\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: Stop evaluation once value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V) of the optimal policy and the optimal value function.        \n",
    "    \"\"\"\n",
    "    \n",
    "    def one_step_lookahead(state, V):\n",
    "        \"\"\"\n",
    "        Function to calculate the value for all actions in a given state.\n",
    "        \n",
    "        Arguments:\n",
    "            state: The state to consider (int)\n",
    "            V: The value to use as an estimator, Vector of length env.nS\n",
    "        \n",
    "        Returns:\n",
    "            A vector of length env.nA containing the expected value of each action.\n",
    "        \"\"\"\n",
    "        A = np.zeros(env.env.nA)\n",
    "        for a in range(env.nA):\n",
    "            for prob, nextState, reward, done in env.P[state][a]:\n",
    "                A[a] += prob * (reward + discount_factor * V[nextState])\n",
    "\n",
    "        return A\n",
    "    \n",
    "    V = np.zeros(env.env.nS)\n",
    "    \n",
    "    numIterations = 0\n",
    "    \n",
    "    while True:\n",
    "        numIterations += 1\n",
    "        delta = 0\n",
    "        \n",
    "        for s in range(env.nS):\n",
    "            qValues = one_step_lookahead(s, V)\n",
    "            newValue = np.max(qValues)\n",
    "            \n",
    "            delta = max(delta, np.abs(newValue - V[s]))\n",
    "            V[s] = newValue\n",
    "        \n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    policy = np.zeros([env.nS, env.nA])\n",
    "    for s in range(env.nS):  #for all states, create deterministic policy\n",
    "        qValues = one_step_lookahead(s,V)\n",
    "        \n",
    "        newAction = np.argmax(qValues)\n",
    "        policy[s][newAction] = 1\n",
    "    \n",
    "    print(numIterations)    \n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "610\n"
     ]
    }
   ],
   "source": [
    "policyVI,valueVI = value_iteration(env, discount_factor=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " ...\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(policyVI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[944.71905319 864.00842358 903.55258694 873.74578494 789.53347837\n",
      " 864.00842358 789.53329112 816.76204048 864.00861084 826.02245805\n",
      " 903.55258694 835.37612279 807.59470382 826.02245805 807.59451656\n",
      " 873.74578494 955.27186266 873.74597793 913.68957728 883.5816994\n",
      " 934.27186266 854.36833935 893.51706107 864.00832709 798.51875678\n",
      " 873.74597793 798.5185714  826.02236156 854.36852473 816.76223347\n",
      " 893.51706107 826.02236156 816.76241885 835.37631577 816.76223347\n",
      " 883.5816994  944.71914403 883.58189046 903.5526815  893.51696554\n",
      " 883.58207398 807.59461113 844.82465595 816.76213794 844.82483948\n",
      " 923.9289605  844.82465595 873.74588241 844.82483948 807.59461113\n",
      " 883.58189046 816.76213794 826.02273614 844.82465595 826.02255261\n",
      " 893.51696554 893.51733638 934.2717709  893.51715469 903.55258694\n",
      " 873.74625324 798.51866502 835.37640939 807.59451656 854.36861466\n",
      " 934.2717709  854.36843297 883.58179589 835.37659109 798.51866502\n",
      " 873.74607155 807.59451656 835.37659109 854.36843297 835.37640939\n",
      " 903.55258694 883.58216302 944.71905319 883.58198314 913.68957728\n",
      " 864.00879071 789.53347837 826.0226453  798.5185714  864.00879071\n",
      " 944.71905319 864.00861084 893.51706107 826.02282518 789.53347837\n",
      " 864.00861084 798.5185714  826.02282518 844.82474864 826.0226453\n",
      " 893.51706107 873.74634139 955.27186266 873.74616331 903.5526815\n",
      " 934.27186266 854.36833935 893.51706107 864.00832709 798.51875678\n",
      " 873.74597793 798.5185714  826.02236156 873.74616331 835.37631577\n",
      " 913.68957728 844.8244649  816.76241885 835.37631577 816.76223347\n",
      " 883.5816994  944.71914403 883.58189046 923.9289605  893.51696554\n",
      " 923.92914403 844.82465595 883.58189046 854.36824382 807.59479466\n",
      " 883.58189046 807.59461113 835.37622025 864.00870168 826.02255261\n",
      " 903.5526815  835.37622025 826.02273614 844.82465595 826.02255261\n",
      " 893.51696554 934.27195259 893.51715469 913.6896709  903.55258694\n",
      " 893.51733638 816.76232709 854.36843297 826.02245805 835.37659109\n",
      " 913.6896709  835.37640939 864.00842358 854.36861466 816.76232709\n",
      " 893.51715469 826.02245805 835.37659109 854.36843297 835.37640939\n",
      " 903.55258694 903.55295407 923.92905319 903.55277419 913.68957728\n",
      " 883.58216302 807.59470382 844.82474864 816.76223347 844.82492851\n",
      " 923.92905319 844.82474864 873.74597793 844.82492851 807.59470382\n",
      " 883.58198314 816.76223347 844.82492851 864.00861084 844.82474864\n",
      " 913.68957728 893.51742453 934.27186266 893.51724645 923.9289605\n",
      " 873.74634139 798.51875678 835.37650115 807.59461113 854.3687028\n",
      " 934.27186266 854.36852473 883.58189046 835.37667923 798.51875678\n",
      " 873.74616331 807.59461113 835.37667923 854.36852473 835.37650115\n",
      " 903.5526815  883.58225028 944.71914403 883.58207398 913.6896709\n",
      " 923.92914403 844.82465595 883.58189046 854.36824382 807.59479466\n",
      " 883.58189046 807.59461113 835.37622025 883.58207398 844.82465595\n",
      " 923.9289605  854.36824382 826.02273614 844.82465595 826.02255261\n",
      " 893.51696554 934.27195259 893.51715469 934.2717709  903.55258694\n",
      " 913.68985259 835.37640939 873.74607155 844.82456139 816.76250878\n",
      " 893.51715469 816.76232709 844.82456139 873.74625324 835.37640939\n",
      " 913.6896709  844.82456139 835.37659109 854.36843297 835.37640939\n",
      " 903.55258694 923.92923307 903.55277419 923.92905319 913.68957728\n",
      " 903.55295407 826.0226453  864.00861084 835.37631577 826.02282518\n",
      " 903.55277419 826.0226453  854.36833935 864.00879071 826.0226453\n",
      " 903.55277419 835.37631577 844.82492851 864.00861084 844.82474864\n",
      " 913.68957728 913.68994074 913.68976266 913.68976266 923.9289605\n",
      " 893.51742453 816.76241885 854.36852473 826.02255261 835.37667923\n",
      " 913.68976266 835.37650115 864.00851815 854.3687028  816.76241885\n",
      " 893.51724645 826.02255261 854.3687028  873.74616331 854.36852473\n",
      " 923.9289605  903.55304133 923.92914403 903.55286503 934.2717709\n",
      " 883.58225028 807.59479466 844.82483948 816.76232709 844.82501578\n",
      " 923.92914403 844.82483948 873.74607155 844.82501578 807.59479466\n",
      " 883.58207398 816.76232709 844.82501578 864.00870168 844.82483948\n",
      " 913.6896709  893.51751091 934.27195259 893.51733638 923.92905319\n",
      " 913.68985259 835.37640939 873.74607155 844.82456139 798.51884671\n",
      " 873.74607155 798.51866502 826.02245805 893.51733638 854.36843297\n",
      " 934.2717709  864.00842358 816.76250878 835.37640939 816.76232709\n",
      " 883.58179589 923.92923307 883.58198314 944.71905319 893.51706107\n",
      " 903.55295407 826.0226453  864.00861084 835.37631577 807.59488369\n",
      " 883.58198314 807.59470382 835.37631577 864.00879071 826.0226453\n",
      " 903.55277419 835.37631577 826.02282518 844.82474864 826.0226453\n",
      " 893.51706107 913.68994074 893.51724645 913.68976266 903.5526815\n",
      " 893.51742453 816.76241885 854.36852473 826.02255261 816.76259692\n",
      " 893.51724645 816.76241885 844.82465595 854.3687028  816.76241885\n",
      " 893.51724645 826.02255261 835.37667923 854.36852473 835.37650115\n",
      " 903.5526815  903.55304133 903.55286503 903.55286503 913.6896709\n",
      " 883.58225028 807.59479466 844.82483948 816.76232709 826.02291244\n",
      " 903.55286503 826.02273614 854.36843297 844.82501578 807.59479466\n",
      " 883.58207398 816.76232709 864.00887797 883.58207398 864.00870168\n",
      " 934.2717709  893.51751091 913.68985259 893.51733638 944.71905319\n",
      " 873.74642778 798.51884671 835.37659109 807.59470382 835.37676562\n",
      " 913.68985259 835.37659109 864.00861084 835.37676562 798.51884671\n",
      " 873.74625324 807.59470382 854.36878919 873.74625324 854.36861466\n",
      " 923.92905319 883.58233581 923.92923307 883.58216302 934.27186266\n",
      " 903.55295407 826.0226453  864.00861084 835.37631577 789.53365824\n",
      " 864.00861084 789.53347837 816.76223347 903.55295407 864.00861084\n",
      " 944.71905319 873.74597793 807.59488369 826.0226453  807.59470382\n",
      " 873.74597793 913.68994074 873.74616331 955.27186266 883.58189046\n",
      " 893.51742453 816.76241885 854.36852473 826.02255261 798.51893486\n",
      " 873.74616331 798.51875678 826.02255261 854.3687028  816.76241885\n",
      " 893.51724645 826.02255261 816.76259692 835.37650115 816.76241885\n",
      " 883.58189046 903.55304133 883.58207398 903.55286503 893.51715469\n",
      " 883.58225028 807.59479466 844.82483948 816.76232709 807.59497095\n",
      " 883.58207398 807.59479466 835.37640939 844.82501578 807.59479466\n",
      " 883.58207398 816.76232709 826.02291244 844.82483948 826.02273614\n",
      " 893.51715469 893.51751091 893.51733638 893.51733638 903.55277419\n",
      " 873.74642778 798.51884671 835.37659109 807.59470382 816.76268331\n",
      " 893.51733638 816.76250878 844.82474864 835.37676562 798.51884671\n",
      " 873.74625324 807.59470382 873.74642778 893.51733638 873.74625324\n",
      " 944.71905319 883.58233581 903.55295407 883.58216302 955.27186266\n",
      " 864.0089635  789.53365824 826.02282518 798.51875678 826.02299796\n",
      " 903.55295407 826.02282518 854.36852473 826.02299796 789.53365824\n",
      " 864.00879071 798.51875678 864.0089635  883.58216302 864.00879071\n",
      " 934.27186266 873.74651245 913.68994074 873.74634139 944.71914403]\n"
     ]
    }
   ],
   "source": [
    "print(valueVI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_learning_train(env,alpha,gamma,epsilon,episodes): \n",
    "    \"\"\"Q Learning Algorithm with epsilon greedy policy\n",
    "\n",
    "    Arguments:\n",
    "        env: Environment \n",
    "        alpha: Learning Rate --> Extent to which our Q-values are being updated in every iteration.\n",
    "        gamma: Discount Rate --> How much importance we want to give to future rewards\n",
    "        epsilon: Probability of selecting random action instead of the 'optimal' action\n",
    "        episodes: No. of episodes to train \n",
    "\n",
    "    Returns:\n",
    "        Q-learning Trained policy\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"Training the agent\"\"\"\n",
    "\n",
    "    \n",
    "    #Initialize Q table here\n",
    "    q_table = np.zeros([env.observation_space.n, env.action_space.n])  \n",
    "    \n",
    "    for i in range(1, episodes+1):\n",
    "        state = env.reset()\n",
    "        # implement the Q-learning algo here\n",
    "\n",
    "       # Start with a random policy\n",
    "    policy = np.ones([env.env.nS, env.env.nA]) / env.env.nA\n",
    "\n",
    "    for state in range(env.env.nS):  #for each states\n",
    "        #Extract the best optimal policy found so far\n",
    "        \n",
    "    \n",
    "    return policy, q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same Policy\n"
     ]
    }
   ],
   "source": [
    "for x in range(len(policyPI[0])):\n",
    "    if not (policyPI[0][x] == policyVI[0][x]).all():\n",
    "        print(\"Not the same Policy\")\n",
    "        break\n",
    "print(\"Same Policy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (binEnv)",
   "language": "python",
   "name": "binenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
